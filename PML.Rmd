---
title: "Machine-Learning Algorithm to Predict Correctness of Weight Lifting Exercises"
subtitle: "Practical Machine Learning - Peer-Graded Assignment"
author: "Carlos Pedrotti"
date: "9/13/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The present document is intended to describe in detail how to build a machine-learning model to predict correctness of the movement in barbell training. 

Data has been originally presented by researchers at [*Groupware@LES*](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har), who acquired data from sensors positioned in athlete's gloves, belt, arm and forearm. While barbell lifting exercises were performed in a variety of ways, those sensors provided accelerometer, gyroscope and magnetometer measures that combined were expected to predict correctness of the movement.

Every code step will be shown and commented. R version 3.4.1 on a macOS 10.12.16, RStudio v. 1.0.143 has been used.

# Strategy

The strategy designed to cuild the machine learning algorithm above mentioned is the following:

* Due to the nature of the data variability, most numeric and continuous, the model selected is the Random Forest.
* First, features containing many NAs will be discarded
* Training data will be sliced in two to allow model validation
* A Random Forest model will be fit to training data
* Model will be tested against tested data and accuracy obtained

<br>
<br>
<br>

# Data Processing

<br>

## Load required packages

At first, the following packages are required to adequately process the data presented in this document:

* data.table v. 1.10.4 (reading data faster and dealing with N/A)
* tidyverse v. 1.1.1 (selecting and filtering data in a faster way)
* caret v. 6.0-77 (preprocessing data)
* randomForest v. 4.6-12 (fit Random Forest models)

```{r}
suppressMessages(suppressWarnings(require(data.table)))
suppressMessages(suppressWarnings(require(tidyverse)))
suppressMessages(suppressWarnings(require(caret)))
suppressMessages(suppressWarnings(require(randomForest)))
suppressMessages(suppressWarnings(require(corrplot)))
```
<br>
<br>
<br>

## Reading data

```{r cache=TRUE}
training <- fread("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  na.strings = c("",NA,"#DIV/0!"))
testing <- fread("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                 na.strings = c("",NA,"#DIV/0!"))
```
<br>
After a first glance at data it is easy to see that many variables have a lot of NAs. The following code intends to count NAs over each column and bring a table of how many columns have NAs and how many have more than 18000 NAs (90+% of all observations). 
<br>
```{r}
na_count <-data.frame(na = sapply(training, function(y) 
                                  sum(length(which(is.na(y))))))
table(na_count!=0)

table(na_count>18000)
```
<br>
<br>
Knowing that 60 columns have no NAs and the remaining 100 variables have more than 
90% of NAs, those variables can be set off the model. Timing and user data is not related to outcome as well, so may not be retained in the final model. The code to obtain the selection is below:
<br>
```{r}
na_count <-data.frame(na = sapply(training, function(y) 
                                  sum(length(which(is.na(y))))))
training <- select(training, which(na_count$na < 1000), -c(1:7))
```
<br>
<br>

## Factorize 'classe' outcome

"Classe" is the outcome and should be factorized.

```{r}
training$classe <- as.factor(training$classe)
```
<br>
<br>

# Data Slicing

Training data will be sliced in a train1 and test1 subgroups toi allow validation inside the trainign data obtained.

```{r}
partition <- createDataPartition(training$classe, p = 0.75, list=F)
train1 <- training[partition,]
test1 <- training[-partition,]         
```
<br>
<br>

# Model Fitting - Random Forests

Due to the numeric characteristic of the data and its variability the mode of choice was the Breinman's Random Forest. After joining all data together in a single data frame the randomForest command is appplied using its default parameters.
<br>
```{r}
fitall <- randomForest(classe ~., data=train1)
```
<br>
<br>

## Cross-validation

The Random Forest model do the cross validation inside the model. Each row is cross validated generating an out-of-bag (oob) error estimate.

```{r}
print(fitall)
```
<br>
<br>

## Testing against sliced train1 subgroup

```{r}
predtest <- predict(fitall, test1)
confusionMatrix(predtest, test1$classe)
```
<br>
<br>

# Prediction Algorithm

The following prediction algorithm process data imputed the same way as the trained data, in order to get the same amount of features and allow proper prediction.

Newdata in the following code is labelled **'testing'** and the result outcome data frame is called **'predictionTable'**


```{r}
testing <- select(testing, which(na_count$na == 0), -c(1:7))
testing$classe <- as.factor(testing$classe)
predictionTable <- data.frame(classe = predict(fitall, testing))
print(predictionTable)
```
<br>
<br>

---
title: "Machine-Learning Algorithm to Predict Correctness of Weight Lifting Exercises"
subtitle: "Practical Machine Learning - Peer-Graded Assignment"
author: "Carlos Pedrotti"
date: "9/13/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The present document is intended to describe in detail how to build a machine-learning model to predict correctness of the movement in barbell training. 

Data has been originally presented by researchers at [*Groupware@LES*](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har), who acquired data from sensors positioned in athlete's gloves, belt, arm and forearm. While barbell lifting exercises were performed in a variety of ways, those sensors provided accelerometer, gyroscope and magnetometer measures that combined were expected to predict correctness of the movement.

Every code step will be shown and commented. R version 3.4.1 on a macOS 10.12.16, RStudio v. 1.0.143 has been used.

# Strategy

The strategy designed to cuild the machine learning algorithm above mentioned is the following:
* Due to the nature of the data variability, most numeric and continuous, the model selected is the Random Forest.
* First, features containing many NAs will be discarded
* Also, due to the use of four different sensors, data acquired from those sensors will be separated and preprocessed individually
* Principal Component Analysis is the method of choice for preprocessing, since it can select the components that contatin most of variance and reduce computing time.
* Data preprocessed from all sensors is joined again for model fitting.
<br>
<br>
<br>

# Data Processing

<br>
<br>

## Load required packages

At first, the following packages are required to adequately process the data presented in this document:

* data.table v. 1.10.4 (reading data faster and dealing with N/A)
* tidyverse v. 1.1.1 (selecting and filtering data in a faster way)
* caret v. 6.0-77 (preprocessing data)
* randomForest v. 4.6-12 (fit Random Forest models)

```{r}
suppressMessages(suppressWarnings(require(data.table)))
suppressMessages(suppressWarnings(require(tidyverse)))
suppressMessages(suppressWarnings(require(caret)))
suppressMessages(suppressWarnings(require(randomForest)))
suppressMessages(suppressWarnings(require(corrplot)))
```
<br>
<br>
<br>

## Reading data

```{r cache=TRUE}
training <- fread("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  na.strings = c("",NA,"#DIV/0!"))
testing <- fread("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                 na.strings = c("",NA,"#DIV/0!"))
```
<br>
After a first glance at data it is easy to see that many variables have a lot of NAs. The following code intends to count NAs over each column and bring a table of how many columns have NAs and how many have more than 18000 NAs (90+% of all observations). 
<br>
```{r}
na_count <-data.frame(na = sapply(training, function(y) 
                                  sum(length(which(is.na(y))))))
table(na_count!=0)

table(na_count>18000)
```
<br>
<br>
Knowing that 60 columns have no NAs and the remaining 100 variables have more than 
90% of NAs, those variables can be set off the model. Timing and user data is not related to outcome as well, so may not be retained in the final model. The code to obtain the selection is below:
<br>
```{r}
na_count <-data.frame(na = sapply(training, function(y) 
                                  sum(length(which(is.na(y))))))
trainx <- select(training, which(na_count$na < 1000), -c(1:7))
```
<br>
<br>

## Factorize 'classe' outcome

"Classe" is the outcome and should be factorized.

```{r}
trainx$classe <- as.factor(trainx$classe)
```
<br>
<br>

## Separating data by sensor

Since each sensor has many measured variables with its own bias it was decided to separate the data into four groups, each one using features collected by one of four sensors: belt, arm, dumbbell and forearm.

```{r}
trainbelt <- select(trainx, contains("belt"))
trainarm <- select(trainx, contains("arm"))
traindumb <- select(trainx, contains("dumbbell"))
trainfore <- select(trainx, contains("forearm"))
```
<br>
<br>

## Preprocessing

Many features withing each group may be highly correlated, since they are collected by the same sensor. An example is the correlation plot of the belt sensor features, shown below.

```{r}
corrplot(cor(trainbelt))
```
<br>
<br>

Therefore, to avoid overfitting it was decided to preprocess each group individually using Principal Component Analysis and select components responsible for more than 90% of total variance. Each preprocessing model was then applied to the respective group.

```{r}
pcabelt <- preProcess(trainbelt, method = "pca", thresh = 0.9)
beltpred <- predict(pcabelt, trainbelt)
pcaarm <- preProcess(trainarm, method = "pca", thresh = 0.9)
armpred <- predict(pcaarm, trainarm)
pcadumb <- preProcess(traindumb, method = "pca", thresh = 0.9)
dumbpred <- predict(pcadumb, traindumb)
pcafore <- preProcess(trainfore, method = "pca", thresh = 0.9)
forepred <- predict(pcafore, trainfore)
```
<br>
<br>

# Model Fitting - Random Forests

Due to the numeric characteristic of the data and its variability the mode of choice was the Breinman's Random Forest. After joining all data together in a single data frame the randomForest command is appplied using its default parameters.
<br>
```{r}
genpred <- data.frame(beltpred, armpred, dumbpred, forepred, 
                      classe = trainx$classe)
fitgen <- randomForest(classe ~., data=genpred)
```
<br>
<br>

## Cross-validation

The Random Forest model do the cross validation inside the model. Each row is cross validated generating an out-of-bag (oob) error estimate.

```{r}
print(fitgen)
```
<br>
<br>

# Prediction Algorithm

The following prediction algorithm process data imputed the same way as the trained data, in order to get preprocessed PCAs at the same level and generate the same amount of features to allow proper prediction.

Newdata in the following code is labelled **'testing'** and the result outcome data frame is called **'tpredall'**


```{r}
testx <- select(testing, which(na_count$na == 0), -c(1:7))
testx$classe <- as.factor(testx$classe)
testbelt <- select(testx, contains("belt"))
testarm <- select(testx, contains("arm"))
testdumb <- select(testx, contains("dumbbell"))
testfore <- select(testx, contains("forearm"))
tpcabelt <- preProcess(testbelt, method = "pca", pcaComp = ncol(beltpred))
tbeltpred <- predict(tpcabelt, testbelt)
tpcaarm <- preProcess(testarm, method = "pca", pcaComp = ncol(armpred))
tarmpred <- predict(tpcaarm, testarm)
tpcadumb <- preProcess(testdumb, method = "pca", pcaComp = ncol(dumbpred))
tdumbpred <- predict(tpcadumb, testdumb)
tpcafore <- preProcess(testfore, method = "pca", pcaComp = ncol(forepred))
tforepred <- predict(tpcafore, testfore)
tgenpred <- data.frame(tbeltpred, tarmpred, tdumbpred, tforepred)


tpredall <- data.frame(classe = predict(fitgen, tgenpred))
```
<br>
<br>
